{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TIieUCrZNwb7"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "njZwHYYHNwb_"
      },
      "outputs": [],
      "source": [
        "input_file = Path('input_image')\n",
        "img_A = Path('Data/img_A') # high qualit images\n",
        "img_B = Path('Data/img_B') # low qualit images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S8soyG4LNwb_"
      },
      "outputs": [],
      "source": [
        "class NoiseAdditionModel(nn.Module):\n",
        "    def __init__(self, noise_type=\"gaussian\", mean=0.0, std=0.1):\n",
        "        \"\"\"\n",
        "        Model to add noise to images.\n",
        "        Args:\n",
        "            noise_type (str): Type of noise ('gaussian', 'salt_pepper').\n",
        "            mean (float): Mean for Gaussian noise.\n",
        "            std (float): Standard deviation for Gaussian noise.\n",
        "        \"\"\"\n",
        "        super(NoiseAdditionModel, self).__init__()\n",
        "        self.noise_type = noise_type\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds noise to input images.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input image tensor of shape (B, C, H, W).\n",
        "        Returns:\n",
        "            torch.Tensor: Noisy images of shape (B, C, H, W).\n",
        "        \"\"\"\n",
        "        if self.noise_type == \"gaussian\":\n",
        "            noise = torch.randn_like(x) * self.std + self.mean\n",
        "            return torch.clamp(x + noise, 0, 1)  # Ensure pixel values are in [0, 1]\n",
        "\n",
        "        elif self.noise_type == \"salt_pepper\":\n",
        "            # Generate salt and pepper noise\n",
        "            prob = 0.05  # Noise probability\n",
        "            salt = torch.rand_like(x) < (prob / 2)\n",
        "            pepper = torch.rand_like(x) < (prob / 2)\n",
        "            noisy_x = x.clone()\n",
        "            noisy_x[salt] = 1.0\n",
        "            noisy_x[pepper] = 0.0\n",
        "            return noisy_x\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported noise type: {self.noise_type}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-xJ3Lq-DNwcA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize the noise addition model\n",
        "noise_model = NoiseAdditionModel(noise_type=\"gaussian\", mean=0.0, std=0.1)\n",
        "\n",
        "# Load an image and preprocess it\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "for _, _, filenames in os.walk(input_file):\n",
        "    for img in filenames:\n",
        "\n",
        "        image = Image.open(f'input_image/{img}').convert(\"RGB\")\n",
        "        image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Add noise to the image\n",
        "        noise_model.eval()\n",
        "        with torch.no_grad():\n",
        "            noisy_image = noise_model(image_tensor)\n",
        "\n",
        "        # Convert the noisy image to a displayable format\n",
        "        to_pil = transforms.ToPILImage()\n",
        "        original_image = to_pil(image_tensor.squeeze(0))\n",
        "        noisy_image_display = to_pil(noisy_image.squeeze(0))\n",
        "\n",
        "        # save the qulait image in the img_A and save low qualit in img_B\n",
        "        original_image.save(f'img_A/{img}')\n",
        "        noisy_image_display.save(f'img_B/{img}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nocH1i5QNwcB"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "class DEGAN_Generator(nn.Module):\n",
        "    def __init__(self, input_channels=3, output_channels=3, biggest_layer=512):\n",
        "        super(DEGAN_Generator, self).__init__()\n",
        "\n",
        "        # Downsampling layers\n",
        "        self.down1 = self.conv_block(input_channels, 64)\n",
        "        self.down2 = self.conv_block(64, 128)\n",
        "        self.down3 = self.conv_block(128, 256)\n",
        "        self.down4 = self.conv_block(256, biggest_layer // 2)\n",
        "        self.down5 = self.conv_block(biggest_layer // 2, biggest_layer)\n",
        "\n",
        "        # Upsampling layers\n",
        "        self.up4 = self.upsample_block(biggest_layer, biggest_layer // 2)\n",
        "        self.up3 = self.upsample_block(biggest_layer // 2, 256)\n",
        "        self.up2 = self.upsample_block(256, 128)\n",
        "        self.up1 = self.upsample_block(128, 64)\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(64, output_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upsample_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Downsampling\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(F.max_pool2d(d1, 2))\n",
        "        d3 = self.down3(F.max_pool2d(d2, 2))\n",
        "        d4 = self.down4(F.max_pool2d(d3, 2))\n",
        "        d5 = self.down5(F.max_pool2d(d4, 2))\n",
        "\n",
        "        # Upsampling\n",
        "        u4 = self.up4(d5) + d4\n",
        "        u3 = self.up3(u4) + d3\n",
        "        u2 = self.up2(u3) + d2\n",
        "        u1 = self.up1(u2) + d1\n",
        "\n",
        "        # Final output\n",
        "        return torch.sigmoid(self.final_conv(u1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y0Qf1HDgNwcB"
      },
      "outputs": [],
      "source": [
        "class DEGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, input_channels=3):\n",
        "        super(DEGAN_Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            self.disc_block(input_channels * 2, 64, normalization=False),\n",
        "            self.disc_block(64, 128),\n",
        "            self.disc_block(128, 256),\n",
        "            self.disc_block(256, 512),\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def disc_block(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1, normalization=True):\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
        "        if normalization:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, img_input, img_target):\n",
        "        # Concatenate degraded and enhanced images along the channel dimension\n",
        "        combined_imgs = torch.cat((img_input, img_target), dim=1)\n",
        "        return self.model(combined_imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IQYG2kEpNwcC"
      },
      "outputs": [],
      "source": [
        "def generator_loss(disc_output, gen_output, target, lambda_l1=100):\n",
        "    adversarial_loss = F.binary_cross_entropy_with_logits(disc_output, torch.ones_like(disc_output))\n",
        "    l1_loss = F.l1_loss(gen_output, target)\n",
        "    return adversarial_loss + lambda_l1 * l1_loss\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = F.binary_cross_entropy_with_logits(real_output, torch.ones_like(real_output))\n",
        "    fake_loss = F.binary_cross_entropy_with_logits(fake_output, torch.zeros_like(fake_output))\n",
        "    return (real_loss + fake_loss) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1EhidaUtNwcD"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize models\n",
        "generator = DEGAN_Generator()\n",
        "discriminator = DEGAN_Discriminator()\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = generator.to(device)\n",
        "discriminator = discriminator.to(device)\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SX_fS_p8NwcD"
      },
      "outputs": [],
      "source": [
        "# Create the data loader\n",
        "# ------------------------\n",
        "# Paired Image Dataset\n",
        "# ------------------------\n",
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, high_quality_dir, low_quality_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Dataset for loading paired high-quality and low-quality images.\n",
        "\n",
        "        Args:\n",
        "            high_quality_dir (str): Path to the directory of high-quality images.\n",
        "            low_quality_dir (str): Path to the directory of low-quality images.\n",
        "            transform (callable, optional): A function/transform to apply to both images.\n",
        "        \"\"\"\n",
        "        self.high_quality_files = sorted(glob(os.path.join(high_quality_dir, '*')))\n",
        "        self.low_quality_files = sorted(glob(os.path.join(low_quality_dir, '*')))\n",
        "        self.transform = transform\n",
        "\n",
        "        if len(self.high_quality_files) != len(self.low_quality_files):\n",
        "            raise ValueError(\"Number of high-quality and low-quality images must be the same.\")\n",
        "\n",
        "        # Ensure files match by filenames (if necessary)\n",
        "        self.high_quality_files = [\n",
        "            f for f in self.high_quality_files\n",
        "            if os.path.basename(f) in {os.path.basename(lp) for lp in self.low_quality_files}\n",
        "        ]\n",
        "        self.low_quality_files = [\n",
        "            f for f in self.low_quality_files\n",
        "            if os.path.basename(f) in {os.path.basename(hp) for hp in self.high_quality_files}\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.high_quality_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        high_quality_path = self.high_quality_files[idx]\n",
        "        low_quality_path = self.low_quality_files[idx]\n",
        "\n",
        "        high_quality_image = Image.open(high_quality_path).convert(\"RGB\")\n",
        "        low_quality_image = Image.open(low_quality_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            high_quality_image = self.transform(high_quality_image)\n",
        "            low_quality_image = self.transform(low_quality_image)\n",
        "\n",
        "        return low_quality_image, high_quality_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8W2T2fAvNwcE",
        "outputId": "e6b42f57-7e5d-4645-b905-01b35526f12a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PairedImageDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5f85532be7eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlow_quality_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Data/img_B'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPairedImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_quality_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_quality_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PairedImageDataset' is not defined"
          ]
        }
      ],
      "source": [
        "high_quality_dir = 'Data/img_A'\n",
        "low_quality_dir = 'Data/img_B'\n",
        "# Load dataset\n",
        "dataset = PairedImageDataset(high_quality_dir, low_quality_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5_s12eeNwcE",
        "outputId": "188231fa-f51c-4787-9961-dfb7107562a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1], Generator Loss: 12.780847549438477, Discriminator Loss: 0.35651612281799316\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    for degraded, target in dataloader:  # Replace `train_dataloader` with your data loader\n",
        "        degraded, target = degraded.to(device), target.to(device)\n",
        "\n",
        "        # Train discriminator\n",
        "        d_optimizer.zero_grad()\n",
        "        real_output = discriminator(degraded, target)\n",
        "        fake_output = discriminator(degraded, generator(degraded).detach())\n",
        "        d_loss = discriminator_loss(real_output, fake_output)\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train generator\n",
        "        g_optimizer.zero_grad()\n",
        "        fake_images = generator(degraded)\n",
        "        fake_output = discriminator(degraded, fake_images)\n",
        "        g_loss = generator_loss(fake_output, fake_images, target)\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Generator Loss: {g_loss.item()}, Discriminator Loss: {d_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s2xTXm-NwcF"
      },
      "outputs": [],
      "source": [
        "# Save the Generator model\n",
        "torch.save(generator.state_dict(), \"noise_generator.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVOdYn_jNwcG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}